{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EcoPrompt Evaluation - Final2\n",
        "## Multi-Tier Model Evaluation with Gemini 2.5 Flash & Apple Silicon Support\n",
        "\n",
        "This notebook implements a comprehensive evaluation framework for comparing different prompt routing and compression strategies across multiple LLM tiers.\n",
        "\n",
        "**Tiers:**\n",
        "- **Tier 1:** Phi-3 Mini (Small, Fast)\n",
        "- **Tier 2:** Mistral 7B (Medium)\n",
        "- **Tier 3:** Gemini 2.5 Flash (Large, API-based)\n",
        "\n",
        "**Scenarios:**\n",
        "- S1: Upper Bound (Tier 3 only)\n",
        "- S2: Lower Bound (Tier 1 only)\n",
        "- S3: Compression (Tier 3 + Compression)\n",
        "- S4: Routing Only\n",
        "- S5: EcoPrompt (Routing + Compression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z2/vh7g_vrx6v547wq5xrwbxt8m0000gn/T/ipykernel_7777/1704554069.py:20: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  import google.generativeai as genai\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import gc\n",
        "import signal\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from codecarbon import EmissionsTracker\n",
        "from llmlingua import PromptCompressor\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration & Global Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "urrent_dir = os.getcwd()\n",
        "# Check current directory first, then parent directory\n",
        "prompt_class_dir = os.path.join(current_dir, \"Nvidia prompt class\")\n",
        "if not os.path.exists(prompt_class_dir):\n",
        "    # Try parent directory\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "    prompt_class_dir = os.path.join(parent_dir, \"Nvidia prompt class\")\n",
        "\n",
        "if os.path.exists(prompt_class_dir) and prompt_class_dir not in sys.path:\n",
        "    sys.path.append(prompt_class_dir)\n",
        "\n",
        "try:\n",
        "    import nvidia_classifier\n",
        "    print(\"done\")\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: Could not import nvidia_classifier from {prompt_class_dir}: {e}\")\n",
        "    nvidia_classifier = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5. Test Gemini API Key\n",
        "Verify that your Gemini API key is configured correctly before running the full experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing Gemini API Key...\n",
            "‚úÖ API Key found: AIzaSyDm0G...5VAKk\n",
            "‚úÖ Gemini API configured successfully\n",
            "\n",
            "üìù Testing with a simple prompt...\n",
            "\n",
            "============================================================\n",
            "ü§ñ GEMINI RESPONSE:\n",
            "============================================================\n",
            "Hello there\n",
            "============================================================\n",
            "\n",
            "‚úÖ SUCCESS! Gemini 2.5 Flash is working correctly! üéâ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test Gemini API configuration\n",
        "print(\"üîç Testing Gemini API Key...\")\n",
        "\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"‚ùå ERROR: GOOGLE_API_KEY not found in environment variables!\")\n",
        "    print(\"\\nüí° Please add your Google API key to the .env file:\")\n",
        "    print(\"   GOOGLE_API_KEY=your_api_key_here\")\n",
        "    print(\"\\nüîó Get your API key from: https://makersuite.google.com/app/apikey\")\n",
        "else:\n",
        "    print(f\"‚úÖ API Key found: {api_key[:10]}...{api_key[-5:]}\")\n",
        "    \n",
        "    # Configure and test Gemini\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "        print(\"‚úÖ Gemini API configured successfully\")\n",
        "        \n",
        "        # Test with a simple prompt\n",
        "        print(\"\\nüìù Testing with a simple prompt...\")\n",
        "        test_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        \n",
        "        response = test_model.generate_content(\n",
        "            \"Say 'Hello! The API is working correctly.' in a friendly way.\",\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                max_output_tokens=50,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ü§ñ GEMINI RESPONSE:\")\n",
        "        print(\"=\"*60)\n",
        "        print(response.text)\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\n‚úÖ SUCCESS! Gemini 2.5 Flash is working correctly! üéâ\\n\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: {e}\")\n",
        "        print(\"\\nüí° Please check:\")\n",
        "        print(\"   1. API key is valid\")\n",
        "        print(\"   2. You have API quota available\")\n",
        "        print(\"   3. Internet connection is stable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ModelManager Class\n",
        "Manages loading, caching, and unloading of LLM models and tokenizers.\n",
        "\n",
        "**Features:**\n",
        "- Auto-detects device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)\n",
        "- Loads and caches models for tier1 (Phi-3), tier2 (Mistral), tier3 (Gemini API)\n",
        "- Handles device-specific optimizations for MPS\n",
        "- Provides unified text generation interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelManager:\n",
        "    \"\"\"Manages loading, caching, and unloading of LLM models and Tokenizers.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            \"tier1\": None,  # Phi-3 Mini\n",
        "            \"tier2\": None,  # Mistral 7B\n",
        "            \"tier3\": None,  # Llama 2 13B\n",
        "            \"nemo\": None\n",
        "        }\n",
        "        self.tokenizers = {\n",
        "            \"tier1\": None,\n",
        "            \"tier2\": None,\n",
        "            \"tier3\": None,\n",
        "            \"nemo\": None\n",
        "        }\n",
        "\n",
        "    def get_device(self):\n",
        "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def load_tier1(self):\n",
        "        \"\"\"Loads Tier 1: Phi-3 Mini\"\"\"\n",
        "        if self.models[\"tier1\"] is not None:\n",
        "            return self.models[\"tier1\"], self.tokenizers[\"tier1\"]\n",
        "\n",
        "        print(\"Loading Tier 1 (Phi-3 Mini)...\")\n",
        "        model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "        return self._load_generic_model(\"tier1\", model_id, use_cache_config=False)\n",
        "\n",
        "    def load_tier2(self):\n",
        "        \"\"\"Loads Tier 2: Mistral 7B\"\"\"\n",
        "        if self.models[\"tier2\"] is not None:\n",
        "            return self.models[\"tier2\"], self.tokenizers[\"tier2\"]\n",
        "\n",
        "        print(\"Loading Tier 2 (Mistral 7B)...\")\n",
        "        model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "        return self._load_generic_model(\"tier2\", model_id)\n",
        "\n",
        "    def load_tier3(self):\n",
        "        \"\"\"Loads Tier 3: Gemini 2.5 Flash (via API)\"\"\"\n",
        "       \n",
        "        if self.models[\"tier3\"] is not None:\n",
        "            return self.models[\"tier3\"], None\n",
        "\n",
        "        print(\"Loading Tier 3 (Gemini 2.5 Flash)...\") \n",
        "        try:\n",
        "            # Initialize Gemini 2.5 Flash model\n",
        "            model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "            self.models[\"tier3\"] = model\n",
        "            print(\"Gemini 2.5 Flash model initialized successfully\")\n",
        "            return model, None\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing Gemini 2.5 Flash: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def _load_generic_model(self, tier_key, model_id, use_auth=False, use_cache_config=True):\n",
        "        token = os.getenv(\"HF_TOKEN\") if use_auth else None\n",
        "        if use_auth and not token:\n",
        "             print(\"Warning: HF_TOKEN not found for authenticated model.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"Checking for local weights for {model_id}...\")\n",
        "            device = self.get_device()\n",
        "            \n",
        "            # For Apple Silicon MPS, we need to handle device mapping differently\n",
        "            if device == \"mps\":\n",
        "                # MPS doesn't support device_map=\"auto\", load to CPU first then move to MPS\n",
        "                try:\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_id, token=token, local_files_only=True)\n",
        "                    model = AutoModelForCausalLM.from_pretrained(\n",
        "                        model_id, \n",
        "                        torch_dtype=torch.float16, \n",
        "                        token=token,\n",
        "                        trust_remote_code=True,\n",
        "                        local_files_only=True,\n",
        "                        low_cpu_mem_usage=True\n",
        "                    )\n",
        "                    model = model.to(device)\n",
        "                    print(f\"Loaded {tier_key} from local cache to MPS.\")\n",
        "                except OSError:\n",
        "                    print(f\"Local weights not found for {tier_key}, downloading from Hub...\")\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "                    model = AutoModelForCausalLM.from_pretrained(\n",
        "                        model_id, \n",
        "                        torch_dtype=torch.float16, \n",
        "                        token=token,\n",
        "                        trust_remote_code=True,\n",
        "                        low_cpu_mem_usage=True\n",
        "                    )\n",
        "                    model = model.to(device)\n",
        "            else:\n",
        "                # CUDA or CPU - use device_map=\"auto\"\n",
        "                try:\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_id, token=token, local_files_only=True)\n",
        "                    model = AutoModelForCausalLM.from_pretrained(\n",
        "                        model_id, \n",
        "                        torch_dtype=torch.float16, \n",
        "                        device_map=\"auto\",\n",
        "                        token=token,\n",
        "                        trust_remote_code=True,\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                    print(f\"Loaded {tier_key} from local cache.\")\n",
        "                except OSError:\n",
        "                    print(f\"Local weights not found for {tier_key}, downloading from Hub...\")\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
        "                    model = AutoModelForCausalLM.from_pretrained(\n",
        "                        model_id, \n",
        "                        torch_dtype=torch.float16, \n",
        "                        device_map=\"auto\",\n",
        "                        token=token,\n",
        "                        trust_remote_code=True\n",
        "                    )\n",
        "            \n",
        "            # Phi-3 specific fix from original code\n",
        "            if not use_cache_config:\n",
        "                 model.generation_config.use_cache = False\n",
        "\n",
        "            self.models[tier_key] = model\n",
        "            self.tokenizers[tier_key] = tokenizer\n",
        "            return model, tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {tier_key}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def get_nemo_model(self):\n",
        "        if nvidia_classifier is None:\n",
        "            return None, None\n",
        "            \n",
        "        if self.models[\"nemo\"] is None:\n",
        "            print(\"Loading NeMo Curator model...\")\n",
        "            try:\n",
        "                self.models[\"nemo\"], self.tokenizers[\"nemo\"] = nvidia_classifier.load_model()\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading NeMo model: {e}\")\n",
        "                return None, None\n",
        "        return self.models[\"nemo\"], self.tokenizers[\"nemo\"]\n",
        "\n",
        "    def unload_model(self, tier):\n",
        "        if self.models.get(tier) is not None:\n",
        "            print(f\"Unloading {tier}...\")\n",
        "            del self.models[tier]\n",
        "            del self.tokenizers[tier]\n",
        "            # Clear cache based on device\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            elif torch.backends.mps.is_available():\n",
        "                torch.mps.empty_cache()\n",
        "            gc.collect()\n",
        "            self.models[tier] = None\n",
        "            self.tokenizers[tier] = None\n",
        "    \n",
        "    def generate(self, tier, prompt):\n",
        "        \"\"\"Generates text using the loaded model for the given tier.\"\"\"\n",
        "        print(f\"[DEBUG] Generating ({tier})... Prompt len: {len(prompt)}\")\n",
        "        \n",
        "        methods = {\"tier1\": self.load_tier1, \"tier2\": self.load_tier2, \"tier3\": self.load_tier3}\n",
        "        if tier not in methods:\n",
        "            return \"Error: Invalid logic tier\"\n",
        "\n",
        "        model_weights = methods[tier]()\n",
        "        \n",
        "        if not model_weights or model_weights[0] is None:\n",
        "            return \"Error: Model loading failed\"\n",
        "            \n",
        "        model, tokenizer = model_weights\n",
        "        \n",
        "        # Special handling for Gemini (tier3)\n",
        "        if tier == \"tier3\":\n",
        "            try:\n",
        "                response = model.generate_content(\n",
        "                    prompt,\n",
        "                    generation_config=genai.types.GenerationConfig(\n",
        "                        max_output_tokens=200,\n",
        "                        temperature=0.9,\n",
        "                    )\n",
        "                )\n",
        "                return response.text.strip()\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating with Gemini: {e}\")\n",
        "                return f\"Error: Gemini generation failed - {e}\"\n",
        "        \n",
        "        # For tier1 and tier2 (transformer models)\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        try:\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages, add_generation_prompt=True, tokenize=True,\n",
        "                return_dict=True, return_tensors=\"pt\"\n",
        "            ).to(model.device)\n",
        "        except Exception:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            \n",
        "        generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "        return generated_text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. IntelligenceEngine Class\n",
        "Handles complexity analysis and prompt compression.\n",
        "\n",
        "**Features:**\n",
        "- Classifies prompt complexity (Easy/Medium/Hard)\n",
        "- Routes prompts to appropriate model tier\n",
        "- Compresses prompts using LLMLingua-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntelligenceEngine:\n",
        "    \"\"\"Handles complexity analysis (Classifier) and Prompt Compression.\"\"\"\n",
        "\n",
        "    def __init__(self, model_manager):\n",
        "        self.mm = model_manager\n",
        "        self.compressor = None\n",
        "\n",
        "    def get_compressor(self):\n",
        "        if self.compressor is None:\n",
        "            print(\"Initializing LLM Lingua-2...\")\n",
        "            try:\n",
        "                self.compressor = PromptCompressor(\n",
        "                    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
        "                    use_llmlingua2=True,\n",
        "                    device_map=\"cpu\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading compressor: {e}\")\n",
        "                return None\n",
        "        return self.compressor\n",
        "\n",
        "    def classify_complexity(self, prompt):\n",
        "        if nvidia_classifier is None:\n",
        "            return \"Medium\"\n",
        "            \n",
        "        try:\n",
        "            model, tokenizer = self.mm.get_nemo_model()\n",
        "            if not model: return \"Medium\"\n",
        "            \n",
        "            result = nvidia_classifier.analyze_prompt(model, tokenizer, prompt)\n",
        "            score = 0.0\n",
        "            if \"prompt_complexity_score\" in result:\n",
        "                try: score = float(result[\"prompt_complexity_score\"][0])\n",
        "                except: score = 0.0\n",
        "            \n",
        "            if score < 0.35: category = \"Easy\"\n",
        "            elif score < 0.65: category = \"Medium\"\n",
        "            else: category = \"Hard\"\n",
        "            \n",
        "            print(f\"[DEBUG] Classifier Score: {score:.4f} -> {category}\")\n",
        "            return category\n",
        "        except Exception as e:\n",
        "            print(f\"Classifier error: {e}\")\n",
        "            return \"Medium\"\n",
        "\n",
        "    def route_prompt(self, category):\n",
        "        if category == \"Easy\": return \"tier1\"\n",
        "        elif category == \"Medium\": return \"tier2\"\n",
        "        elif category == \"Hard\": return \"tier3\"\n",
        "        return \"tier2\"\n",
        "\n",
        "    def compress_prompt(self, prompt):\n",
        "        compressor = self.get_compressor()\n",
        "        if not compressor:\n",
        "            return {\"text\": prompt, \"rate\": 1.0, \"init\": 0, \"final\": 0}\n",
        "            \n",
        "        try:\n",
        "            token_count = len(compressor.tokenizer.encode(prompt))\n",
        "            # Heuristic\n",
        "            slope = 9.5 / 8000\n",
        "            ratio = 2.5 + (token_count - 2000) * slope\n",
        "            if ratio < 1.0: ratio = 1.0\n",
        "            \n",
        "            # Min compression ratio constraint\n",
        "            if ratio < 2.0 and ratio > 1.0: ratio = 2.0\n",
        "            \n",
        "            if ratio == 1.0:\n",
        "                 return {\"text\": prompt, \"rate\": 1.0, \"init\": token_count, \"final\": token_count}\n",
        "\n",
        "            rate = 1 / ratio\n",
        "            res = compressor.compress_prompt(prompt, rate=rate, force_tokens=['\\n', '?'])\n",
        "            \n",
        "            compressed_text = res['compressed_prompt'] if isinstance(res, dict) else res\n",
        "            final_tokens = len(compressor.tokenizer.encode(compressed_text))\n",
        "            \n",
        "            return {\n",
        "                \"text\": compressed_text,\n",
        "                \"rate\": rate,\n",
        "                \"init\": token_count,\n",
        "                \"final\": final_tokens\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Compression failed: {e}\")\n",
        "            return {\"text\": prompt, \"rate\": 1.0, \"init\": 0, \"final\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DatasetLoader Class\n",
        "Handles loading datasets from local files and formatting prompts.\n",
        "\n",
        "**Supported Datasets:**\n",
        "- GLUE (MNLI, SST-2)\n",
        "- SQuAD v2\n",
        "- CNN/DailyMail\n",
        "- GSM8K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DatasetLoader:\n",
        "    \"\"\"Handles loading of datasets from local files and formatting prompts.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_root):\n",
        "        self.data_root = data_root\n",
        "\n",
        "    def get_local_path(self, ds_name, subset, split):\n",
        "        # Map dataset args to local dataset directories and their splits\n",
        "        # Returns tuple of (dataset_dir, split_name)\n",
        "        paths = {\n",
        "            (\"glue\", \"mnli\"): (os.path.join(self.data_root, \"data\", \"NLI_MNLI\"), \"validation_matched\"),\n",
        "            (\"glue\", \"sst2\"): (os.path.join(self.data_root, \"data\", \"SST-2\"), \"validation\"),\n",
        "            (\"squad_v2\", None): (os.path.join(self.data_root, \"data\", \"SQuAD_v2\"), \"validation\"),\n",
        "            (\"cnn_dailymail\", \"3.0.0\"): (os.path.join(self.data_root, \"data\", \"CNN_DailyMail\"), \"test\"),\n",
        "            (\"gsm8k\", \"main\"): (os.path.join(self.data_root, \"data\", \"GSM8K\"), \"test\")\n",
        "        }\n",
        "        return paths.get((ds_name, subset))\n",
        "\n",
        "    def load(self, ds_name, subset, split, samples):\n",
        "        path_info = self.get_local_path(ds_name, subset, split)\n",
        "        if not path_info:\n",
        "            print(f\"Unknown local path for {ds_name}/{subset}\")\n",
        "            return []\n",
        "\n",
        "        dataset_dir, split_name = path_info\n",
        "        \n",
        "        try:\n",
        "            # Load the dataset dictionary from the local directory using load_from_disk\n",
        "            # HuggingFace datasets cache with dataset_dict.json at root\n",
        "            ds_dict = load_from_disk(dataset_dir)\n",
        "            \n",
        "            # Access the specific split\n",
        "            if split_name not in ds_dict:\n",
        "                print(f\"Split '{split_name}' not found. Available: {list(ds_dict.keys())}\")\n",
        "                return []\n",
        "            \n",
        "            ds = ds_dict[split_name]\n",
        "            ds = ds.select(range(min(len(ds), samples)))\n",
        "            return ds\n",
        "        except Exception as e:\n",
        "            print(f\"Dataset load error ({dataset_dir}, split={split_name}): {e}\")\n",
        "            return []\n",
        "\n",
        "    def format_sample(self, item, ds_name, subset):\n",
        "        \"\"\"Returns (prompt, reference_answer, dataset_display_name)\"\"\"\n",
        "        prompt = \"\"\n",
        "        ref = \"\"\n",
        "        \n",
        "        # Unique display name to avoid collision\n",
        "        display_name = f\"{ds_name}/{subset}\" if subset else ds_name\n",
        "\n",
        "        if ds_name == \"glue\" and subset == \"mnli\":\n",
        "            prompt = f\"Premise: {item['premise']}\\nHypothesis: {item['hypothesis']}\\nlabel (entailment, neutral, contradiction):\"\n",
        "            ref = item['label']\n",
        "        elif ds_name == \"glue\" and subset == \"sst2\":\n",
        "            prompt = f\"Sentence: {item['sentence']}\\nSentiment (positive, negative):\"\n",
        "            ref = item['label']\n",
        "        elif ds_name == \"squad_v2\":\n",
        "            prompt = f\"Context: {item['context']}\\nQuestion: {item['question']}\\nAnswer:\"\n",
        "            ref = item['answers']\n",
        "        elif ds_name == \"cnn_dailymail\":\n",
        "            prompt = f\"Summarize:\\n{item['article'][:2000]}\"\n",
        "            ref = item['highlights']\n",
        "        elif ds_name == \"gsm8k\":\n",
        "            prompt = f\"Question: {item['question']}\\nThink step by step:\"\n",
        "            ref = item['answer']\n",
        "            \n",
        "        return prompt, ref, display_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluator Class\n",
        "Static evaluation metrics for different tasks.\n",
        "\n",
        "**Metrics:**\n",
        "- Accuracy (MNLI, SST-2)\n",
        "- Exact Match (SQuAD, GSM8K)\n",
        "- ROUGE-L (CNN/DailyMail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    \"\"\"Static evaluation metrics.\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def evaluate(output, reference, ds_name, subset):\n",
        "        if ds_name == \"glue\":\n",
        "            if subset == \"mnli\": return Evaluator.mnli(output, reference), \"accuracy\"\n",
        "            if subset == \"sst2\": return Evaluator.sst2(output, reference), \"accuracy\"\n",
        "        elif ds_name == \"squad_v2\":\n",
        "            return Evaluator.squad(output, reference), \"EM\"\n",
        "        elif ds_name == \"cnn_dailymail\":\n",
        "            return Evaluator.rouge(output, reference), \"ROUGE-L\"\n",
        "        elif ds_name == \"gsm8k\":\n",
        "            return Evaluator.gsm8k(output, reference), \"EM\"\n",
        "        return 0.0, \"unknown\"\n",
        "\n",
        "    @staticmethod\n",
        "    def mnli(pred, label):\n",
        "        pred = pred.lower()\n",
        "        map_ = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "        lbl = map_.get(label, \"\") if isinstance(label, int) else str(label).lower()\n",
        "        \n",
        "        if \"entailment\" in pred: p = \"entailment\"\n",
        "        elif \"neutral\" in pred: p = \"neutral\"\n",
        "        elif \"contradiction\" in pred: p = \"contradiction\"\n",
        "        else: p = \"unknown\"\n",
        "        return 1 if p == lbl else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def sst2(pred, label):\n",
        "        pred = pred.lower()\n",
        "        map_ = {0: \"negative\", 1: \"positive\"}\n",
        "        lbl = map_.get(label, \"\") if isinstance(label, int) else str(label).lower()\n",
        "        \n",
        "        if \"positive\" in pred: p = \"positive\"\n",
        "        elif \"negative\" in pred: p = \"negative\"\n",
        "        else: p = \"unknown\"\n",
        "        return 1 if p == lbl else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def squad(pred, answers):\n",
        "        candidates = answers.get('text', [])\n",
        "        if not candidates: return 0\n",
        "        \n",
        "        def normalize(s):\n",
        "            import string\n",
        "            s = ''.join(ch for ch in s.lower() if ch not in set(string.punctuation))\n",
        "            s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
        "            return ' '.join(s.split())\n",
        "\n",
        "        pred_norm = normalize(pred)\n",
        "        return max([1 if normalize(a) == pred_norm else 0 for a in candidates])\n",
        "\n",
        "    @staticmethod\n",
        "    def rouge(pred, ref):\n",
        "        if not pred: return 0.0\n",
        "        try:\n",
        "            scorer = evaluate.load(\"rouge\")\n",
        "            res = scorer.compute(predictions=[pred], references=[ref])\n",
        "            return res.get('rougeL', 0.0)\n",
        "        except: return 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def gsm8k(pred, ref_str):\n",
        "        gold = ref_str.split(\"####\")[-1].strip()\n",
        "        nums = re.findall(r'-?\\d+\\.?\\d*', pred)\n",
        "        pred_num = nums[-1] if nums else \"\"\n",
        "        try:\n",
        "            return 1 if abs(float(pred_num) - float(gold)) < 1e-5 else 0\n",
        "        except: return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ExperimentRunner Class\n",
        "Main orchestrator for running experiments.\n",
        "\n",
        "**Features:**\n",
        "- Manages multiple scenarios and datasets\n",
        "- Tracks carbon emissions with CodeCarbon\n",
        "- Saves results incrementally to prevent data loss\n",
        "- Handles graceful shutdown on interruption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"Main Orchestrator.\"\"\"\n",
        "    \n",
        "    def __init__(self, args, data_root):\n",
        "        self.args = args\n",
        "        self.mm = ModelManager()\n",
        "        self.ie = IntelligenceEngine(self.mm)\n",
        "        self.loader = DatasetLoader(data_root)\n",
        "        self.results = []\n",
        "        self.csv_initialized = False\n",
        "        self.interrupted = False\n",
        "        \n",
        "        self.scenarios = [\n",
        "            {\"id\": \"S1\", \"name\": \"Upper Bound (Tier 3)\", \"routing\": False, \"compression\": False, \"fixed\": \"tier3\"},\n",
        "            {\"id\": \"S2\", \"name\": \"Lower Bound (Tier 1)\", \"routing\": False, \"compression\": False, \"fixed\": \"tier1\"},\n",
        "            {\"id\": \"S3\", \"name\": \"Compression (Tier 3 + Comp)\", \"routing\": False, \"compression\": True, \"fixed\": \"tier3\"},\n",
        "            {\"id\": \"S4\", \"name\": \"Routing Only\", \"routing\": True, \"compression\": False, \"fixed\": None},\n",
        "            {\"id\": \"S5\", \"name\": \"EcoPrompt (Routing + Comp)\", \"routing\": True, \"compression\": True, \"fixed\": None},\n",
        "        ]\n",
        "        \n",
        "        # Setup signal handlers for graceful shutdown\n",
        "        signal.signal(signal.SIGINT, self._signal_handler)\n",
        "        signal.signal(signal.SIGTERM, self._signal_handler)\n",
        "    \n",
        "    def _signal_handler(self, signum, frame):\n",
        "        \"\"\"Handle termination signals gracefully.\"\"\"\n",
        "        print(\"\\n\\n‚ö†Ô∏è  Process interrupted! Saving results before exit...\")\n",
        "        self.interrupted = True\n",
        "        self._save_results()\n",
        "        print(\"‚úÖ Results saved. Exiting gracefully.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    def run(self):\n",
        "        # 1. Filter Datasets\n",
        "        all_dsets = [\n",
        "            (\"glue\", \"mnli\", \"validation_matched\"), \n",
        "            (\"glue\", \"sst2\", \"validation\"),\n",
        "            (\"squad_v2\", None, \"validation\"),\n",
        "            (\"cnn_dailymail\", \"3.0.0\", \"test\"),#why not\n",
        "            (\"gsm8k\", \"main\", \"test\")\n",
        "        ]\n",
        "        \n",
        "        target_dsets = []\n",
        "        if \"all\" in self.args.datasets:\n",
        "            target_dsets = all_dsets\n",
        "        else:\n",
        "            for d in self.args.datasets:\n",
        "                for cand in all_dsets:\n",
        "                    name_full = f\"{cand[0]}/{cand[1]}\" if cand[1] else cand[0]\n",
        "                    if d == cand[0] or d == name_full:\n",
        "                        target_dsets.append(cand)\n",
        "\n",
        "        # 2. Filter Scenarios\n",
        "        active_scenarios = self.scenarios\n",
        "        if \"all\" not in self.args.scenarios:\n",
        "             active_scenarios = [s for s in self.scenarios if s[\"id\"] in self.args.scenarios]\n",
        "\n",
        "        # 3. CodeCarbon\n",
        "        tracker = None\n",
        "        if not self.args.no_tracking:\n",
        "            tracker = EmissionsTracker(project_name=\"ecoprompt\", measure_power_secs=1, save_to_file=False, log_level=\"error\")\n",
        "\n",
        "        print(f\"Starting Experiment: {self.args.samples} samples...\")\n",
        "\n",
        "        # 4. Main Loop\n",
        "        for ds_name, subset, split in target_dsets:\n",
        "            if self.interrupted:\n",
        "                break\n",
        "                \n",
        "            print(f\"\\ndataset: {ds_name} ({subset or ''})\")\n",
        "            data = self.loader.load(ds_name, subset, split, self.args.samples)\n",
        "            if not data: continue\n",
        "            \n",
        "            for i, item in enumerate(tqdm(data)):\n",
        "                if self.interrupted:\n",
        "                    break\n",
        "                    \n",
        "                prompt, ref, unique_ds_name = self.loader.format_sample(item, ds_name, subset)\n",
        "                \n",
        "                # Analyze once per sample\n",
        "                category = self.ie.classify_complexity(prompt)\n",
        "                \n",
        "                for sc in active_scenarios:\n",
        "                    if self.interrupted:\n",
        "                        break\n",
        "                    self._run_scenario(sc, i, prompt, ref, category, unique_ds_name, tracker)\n",
        "\n",
        "        self._save_results()\n",
        "\n",
        "    def _run_scenario(self, sc, idx, prompt, ref, category, ds_name, tracker):\n",
        "        # 1. Determine Model\n",
        "        tier = \"\"\n",
        "        if sc[\"routing\"]:\n",
        "            tier = self.ie.route_prompt(category)\n",
        "        else:\n",
        "            tier = sc[\"fixed\"]\n",
        "            \n",
        "        model_display = f\"{tier} ({category})\" if sc[\"routing\"] else tier\n",
        "\n",
        "        # 2. Compression\n",
        "        final_prompt = prompt\n",
        "        c_stats = {\"rate\": 1.0, \"init\": len(prompt.split()), \"final\": len(prompt.split())}\n",
        "        \n",
        "        if sc[\"compression\"]:\n",
        "            res = self.ie.compress_prompt(prompt)\n",
        "            final_prompt = res[\"text\"]\n",
        "            c_stats = {\"rate\": res[\"rate\"], \"init\": res[\"init\"], \"final\": res[\"final\"]}\n",
        "\n",
        "        # 3. Generation & Emissions\n",
        "        emissions = 0.0\n",
        "        output = \"\"\n",
        "        try:\n",
        "            if tracker: tracker.start()\n",
        "            output = self.mm.generate(tier, final_prompt)\n",
        "            if tracker: emissions = tracker.stop()\n",
        "            print(\"ek prompt hogaya\")\n",
        "            print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Gen Error: {e}\")\n",
        "            output = \"Error\"\n",
        "            if tracker and tracker._start_time: tracker.stop()\n",
        "\n",
        "        # 4. Score\n",
        "        score = 0.0\n",
        "        stype = \"acc\"\n",
        "        ds_raw = ds_name.split(\"/\")[0] # 'glue/mnli' -> 'glue' for evaluator dispatch\n",
        "        subset = ds_name.split(\"/\")[1] if \"/\" in ds_name else None\n",
        "        \n",
        "        if output != \"Error\":\n",
        "             score, stype = Evaluator.evaluate(output, ref, ds_raw, subset)\n",
        "\n",
        "        # 5. Record\n",
        "        row = {\n",
        "            \"scenario_id\": sc[\"id\"],\n",
        "            \"scenario_name\": sc[\"name\"],\n",
        "            \"dataset\": ds_name, # Now contains unique name (e.g. glue/mnli)\n",
        "            \"sample_index\": idx,\n",
        "            \"prompt_complexity\": category,\n",
        "            \"model_used\": model_display,\n",
        "            \"original_prompt_len\": c_stats[\"init\"],\n",
        "            \"compressed_prompt_len\": c_stats[\"final\"],\n",
        "            \"compression_rate\": c_stats[\"rate\"],\n",
        "            \"carbon_kg\": emissions,\n",
        "            \"accuracy_score\": score,\n",
        "            \"score_type\": stype,\n",
        "            \"output_excerpt\": output[:100].replace(\"\\n\", \" \"),\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "        self.results.append(row)\n",
        "        \n",
        "        # Immediately save to CSV to prevent data loss\n",
        "        self._append_to_csv(row)\n",
        "\n",
        "    def _initialize_csv(self):\n",
        "        \"\"\"Initialize CSV file with headers if it doesn't exist or is empty.\"\"\"\n",
        "        if not self.csv_initialized:\n",
        "            fieldnames = [\n",
        "                \"scenario_id\", \"scenario_name\", \"dataset\", \"sample_index\",\n",
        "                \"prompt_complexity\", \"model_used\", \"original_prompt_len\",\n",
        "                \"compressed_prompt_len\", \"compression_rate\", \"carbon_kg\",\n",
        "                \"accuracy_score\", \"score_type\", \"output_excerpt\", \"timestamp\"\n",
        "            ]\n",
        "            \n",
        "            # Check if file exists and has content\n",
        "            file_exists = os.path.exists(self.args.output_csv)\n",
        "            if file_exists:\n",
        "                with open(self.args.output_csv, 'r') as f:\n",
        "                    file_empty = len(f.read().strip()) == 0\n",
        "            else:\n",
        "                file_empty = True\n",
        "            \n",
        "            # Write headers if file is new or empty\n",
        "            if not file_exists or file_empty:\n",
        "                with open(self.args.output_csv, 'w', newline='') as f:\n",
        "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                    writer.writeheader()\n",
        "                print(f\"Initialized CSV file: {self.args.output_csv}\")\n",
        "            \n",
        "            self.csv_initialized = True\n",
        "    \n",
        "    def _append_to_csv(self, row):\n",
        "        \"\"\"Append a single row to the CSV file immediately.\"\"\"\n",
        "        self._initialize_csv()\n",
        "        \n",
        "        try:\n",
        "            with open(self.args.output_csv, 'a', newline='') as f:\n",
        "                fieldnames = [\n",
        "                    \"scenario_id\", \"scenario_name\", \"dataset\", \"sample_index\",\n",
        "                    \"prompt_complexity\", \"model_used\", \"original_prompt_len\",\n",
        "                    \"compressed_prompt_len\", \"compression_rate\", \"carbon_kg\",\n",
        "                    \"accuracy_score\", \"score_type\", \"output_excerpt\", \"timestamp\"\n",
        "                ]\n",
        "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                writer.writerow(row)\n",
        "                f.flush()  # Force write to disk immediately\n",
        "                os.fsync(f.fileno())  # Ensure OS buffers are written to disk\n",
        "        except Exception as e:\n",
        "            print(f\"Error appending to CSV: {e}\")\n",
        "    \n",
        "    def _save_results(self):\n",
        "        \"\"\"Generate summary and pivot tables from collected results.\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.results)\n",
        "        print(f\"\\nMain results already saved to {self.args.output_csv}\")\n",
        "\n",
        "        # Summary\n",
        "        summary = df.groupby([\"scenario_id\", \"scenario_name\", \"dataset\"])[\"accuracy_score\"].mean().reset_index()\n",
        "        summary_path = self.args.output_csv.replace(\".csv\", \"_summary.csv\")\n",
        "        summary.to_csv(summary_path, index=False)\n",
        "        print(f\"Saved summary to {summary_path}\")\n",
        "\n",
        "        # Pivot (Wide Format)\n",
        "        try:\n",
        "            pivot = df.pivot_table(\n",
        "                index=['dataset', 'sample_index'],\n",
        "                columns='scenario_id',\n",
        "                values=['accuracy_score', 'carbon_kg', 'model_used'],\n",
        "                aggfunc='first'\n",
        "            )\n",
        "            # Flatten columns\n",
        "            pivot.columns = [f\"{col[1]}_{col[0]}\" for col in pivot.columns]\n",
        "            pivot.reset_index(inplace=True)\n",
        "            \n",
        "            pivot_path = self.args.output_csv.replace(\".csv\", \"_per_prompt.csv\")\n",
        "            pivot.to_csv(pivot_path, index=False)\n",
        "            print(f\"Saved pivot comparison to {pivot_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Pivot error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Main Function\n",
        "Argument parsing and experiment initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--samples\", type=int, default=5)\n",
        "    parser.add_argument(\"--output_csv\", type=str, default=\"evaluation_scenarios_results.csv\")\n",
        "    parser.add_argument(\"--no_tracking\", action=\"store_true\")\n",
        "    parser.add_argument(\"--datasets\", nargs=\"+\", default=[\"all\"])\n",
        "    parser.add_argument(\"--scenarios\", nargs=\"+\", default=[\"all\"])\n",
        "    parser.add_argument(\"--data_root\", type=str, default=None, help=\"Root directory containing data folder\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Determine project root - auto-detect data directory location\n",
        "    if args.data_root:\n",
        "        project_root = args.data_root\n",
        "    else:\n",
        "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "        \n",
        "        # Try multiple locations for data directory\n",
        "        # 1. Parent directory (for scenarios_evaluation subfolder structure)\n",
        "        parent_dir = os.path.dirname(script_dir)\n",
        "        if os.path.exists(os.path.join(parent_dir, \"data\")):\n",
        "            project_root = parent_dir\n",
        "        # 2. Current script directory (for co-located scripts)\n",
        "        elif os.path.exists(os.path.join(script_dir, \"data\")):\n",
        "            project_root = script_dir\n",
        "        # 3. Home directory (for GPU server setup)\n",
        "        elif os.path.exists(os.path.join(os.path.expanduser(\"~\"), \"data\")):\n",
        "            project_root = os.path.expanduser(\"~\")\n",
        "        else:\n",
        "            # Fallback to parent directory\n",
        "            project_root = parent_dir\n",
        "    \n",
        "    print(f\"[INFO] Using data root: {project_root}\")\n",
        "    print(f\"[INFO] Looking for data in: {os.path.join(project_root, 'data')}\")\n",
        "    \n",
        "    #runner = ExperimentRunner(args, data_root=project_root)\n",
        "    runner.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Run Experiment\n",
        "Configure and execute the evaluation.\n",
        "\n",
        "### Configuration Options:\n",
        "- `samples`: Number of samples per dataset (default: 5)  \n",
        "- `datasets`: List of datasets to evaluate (default: all)\n",
        "- `scenarios`: List of scenarios to run (default: all)\n",
        "- `no_tracking`: Disable carbon tracking (default: False)\n",
        "- `output_csv`: Output CSV filename (default: evaluation_scenarios_results.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Run on GSM8K with 10 samples\n",
        "class Args:\n",
        "    samples = 10\n",
        "    output_csv = \"evaluation_scenarios_results.csv\"\n",
        "    no_tracking = False\n",
        "    datasets = [\"gsm8k\"]\n",
        "    scenarios = [\"all\"]\n",
        "    data_root = None\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Determine project root\n",
        "script_dir = os.getcwd()\n",
        "parent_dir = os.path.dirname(script_dir)\n",
        "\n",
        "if os.path.exists(os.path.join(parent_dir, \"data\")):\n",
        "    project_root = parent_dir\n",
        "elif os.path.exists(os.path.join(script_dir, \"data\")):\n",
        "    project_root = script_dir\n",
        "else:\n",
        "    project_root = parent_dir\n",
        "\n",
        "print(f\"[INFO] Using data root: {project_root}\")\n",
        "print(f\"[INFO] Looking for data in: {os.path.join(project_root, 'data')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Execute Experiment\n",
        "**Run the cell below to start the evaluation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and run the experiment\n",
        "runner = ExperimentRunner(args, data_root=project_root)\n",
        "runner.run()\n",
        "\n",
        "print(\"\\n‚úÖ Experiment complete!\")\n",
        "print(f\"Results saved to: {args.output_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. View Results\n",
        "Load and display the evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load results\n",
        "results_df = pd.read_csv(\"evaluation_scenarios_results.csv\")\n",
        "summary_df = pd.read_csv(\"evaluation_scenarios_results_summary.csv\")\n",
        "pivot_df = pd.read_csv(\"evaluation_scenarios_results_per_prompt.csv\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SUMMARY RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "display(summary_df)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED RESULTS (First 10 rows)\")\n",
        "print(\"=\" * 60)\n",
        "display(results_df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PER-PROMPT COMPARISON (First 5 rows)\")\n",
        "print(\"=\" * 60)\n",
        "display(pivot_df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Visualize Results\n",
        "Create plots to compare scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Accuracy by Scenario\n",
        "plt.subplot(1, 2, 1)\n",
        "summary_pivot = summary_df.pivot(index='dataset', columns='scenario_name', values='accuracy_score')\n",
        "summary_pivot.plot(kind='bar', ax=plt.gca())\n",
        "plt.title('Accuracy by Scenario and Dataset')\n",
        "plt.xlabel('Dataset')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.legend(title='Scenario', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot 2: Carbon Emissions (if tracked)\n",
        "plt.subplot(1, 2, 2)\n",
        "if 'carbon_kg' in results_df.columns and results_df['carbon_kg'].sum() > 0:\n",
        "    carbon_summary = results_df.groupby('scenario_name')['carbon_kg'].sum()\n",
        "    carbon_summary.plot(kind='bar', color='green', alpha=0.7, ax=plt.gca())\n",
        "    plt.title('Carbon Emissions by Scenario')\n",
        "    plt.xlabel('Scenario')\n",
        "    plt.ylabel('Total Carbon (kg CO2)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Carbon tracking disabled', \n",
        "             ha='center', va='center', fontsize=12)\n",
        "    plt.title('Carbon Emissions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
